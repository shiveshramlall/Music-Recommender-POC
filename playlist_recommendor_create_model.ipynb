{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All imports needed for notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import Dict, Text\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "# -- local files\n",
    "from modules import spotify_methods as sm\n",
    "from modules import content_recommender as cr\n",
    "from modules import recommender_methods as rm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Dataset Preparation\n",
    "We need to prepare our 1 000 000 playlists into a suitable format for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"C:\\\\Users\\\\Shivesh\\\\Documents\\\\Repos\\\\Personal\\\\Projects\\\\music-recommendor\\\\millionplaylistsubset\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect dataset songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"C:\\\\Users\\\\Shivesh\\\\Documents\\\\Repos\\\\Personal\\\\Projects\\\\music-recommendor\\\\millionplaylistsubset\"\n",
    "\n",
    "playlist_id_ref = {}\n",
    "playlist_users_ref = {}\n",
    "playlist_users_ref['anonymous'] = {}\n",
    "\n",
    "files_count = 0\n",
    "playlist_count = 0\n",
    "\n",
    "filenames = os.listdir(dataset_path)\n",
    "\n",
    "collect_songs_dict = {}\n",
    "max_id = 0\n",
    "for filename in sorted(filenames):\n",
    "\n",
    "    if filename.startswith(\"mpd.slice.\") and filename.endswith(\".json\"):\n",
    "\n",
    "        fullpath = os.sep.join((dataset_path, filename))\n",
    "        f = open(fullpath)\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        mpd_slice = json.loads(js)\n",
    "\n",
    "        for playlist in mpd_slice[\"playlists\"]:\n",
    "            \n",
    "            collect_songs_dict[playlist.get(\"pid\")] = {}\n",
    "            tracks = playlist['tracks']\n",
    "\n",
    "            temp_list = []\n",
    "            for track in tracks:\n",
    "\n",
    "                temp_list += [track.get(\"track_name\").lower() +\" --- \"+ track.get(\"artist_name\").lower()] \n",
    "\n",
    "            collect_songs_dict[playlist.get(\"pid\")] = temp_list\n",
    "\n",
    "            playlist_id_ref[playlist.get(\"pid\")]  = {\"name\": playlist.get(\"name\").lower(), \"owner\": \"anonymous\"}\n",
    "            playlist_users_ref['anonymous'].update({playlist.get(\"pid\"): playlist.get(\"name\").lower()})\n",
    "\n",
    "            playlist_count += 1\n",
    "\n",
    "            \n",
    "            max_id +=1\n",
    "\n",
    "        files_count += 1\n",
    "\n",
    "print(f'''Files read: {files_count}''')\n",
    "print(f'''Next available playlist ID: {max_id}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_songs = []\n",
    "for pid, tracks in collect_songs_dict.items():\n",
    "    playlist_songs += [[str(pid), track] for track in tracks]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add additonal spotify playlists to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_users = ['tmmb1pzyny780errk5d7vuz6r', '1yoo33uf0br9bvkmn1wlahjgr', '22q7rezq5nqdwct7ujf7knkua', 'mgscx8hs8egcjamt31rfkm8ww']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect playlist ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_playlists = {}\n",
    "for su in spotify_users:\n",
    "\n",
    "    playlist_users_ref[su] = {}\n",
    "    user_playlists = sm.collect_user_playlists(su)\n",
    "    extra_playlists[su] = user_playlists\n",
    "\n",
    "print(f'''No of playlists: {len(extra_playlists)}''')\n",
    "print(\"Playlist Users and IDs:\")\n",
    "print((extra_playlists))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all tracks for the above playlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_playlist_songs = []\n",
    "\n",
    "for ep_user, ep_playlists in extra_playlists.items():\n",
    "\n",
    "    collect_tracks, new_id_ref, new_users_ref = sm.collect_tracks(ep_playlists, ep_user, playlist_count)\n",
    "    \n",
    "    playlist_id_ref.update(new_id_ref)\n",
    "    playlist_users_ref.update(new_users_ref)\n",
    "\n",
    "    extra_playlist_songs += collect_tracks\n",
    "    playlist_count += len(ep_playlists)\n",
    "\n",
    "print(f'''Total playlists collected: {playlist_count}''')\n",
    "print(f'''Extra playlists collected from spotify: {len(extra_playlist_songs)}''')\n",
    "playlist_songs += extra_playlist_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./model/extra_playlist_songs.json', 'w') as fp:\n",
    "    json.dump(extra_playlist_songs, fp, indent=2)\n",
    "\n",
    "with open('./model/playlists_id_ref.json', 'w') as fp:\n",
    "    json.dump(playlist_id_ref, fp, indent=2)\n",
    "\n",
    "with open('./model/playlists_users_ref.json', 'w') as fp:\n",
    "    json.dump(playlist_users_ref, fp, indent=2)\n",
    "\n",
    "filename = os.getcwd() + \"\\\\model\" + \"\\\\training_data.pkl\"\n",
    "with open(filename, 'wb') as fp:\n",
    "    pk.dump(playlist_songs, fp)\n",
    "\n",
    "# with open('./model/training_data.json', 'w') as fp:\n",
    "#     json.dump(playlist_songs, fp, indent=2)\n",
    "\n",
    "print(\"Saved references for playlist IDs names and owners in ./model/\")\n",
    "print(\"Saved training data within ./model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Training Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if Tensorflow is detecting my GPU\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_songs = tf.data.Dataset.from_tensor_slices(playlist_songs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add columns names (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "playlist_songs = playlist_songs.map(lambda x: {\n",
    "    \"pid\": x[0],\n",
    "    \"song_name\": x[1],\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(playlist_songs))\n",
    "pl_songs_count = len(playlist_songs)\n",
    "pl_songs_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(playlist_songs)\n",
    "\n",
    "for r in playlist_songs:\n",
    "    print(r)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to have the songs stored seperately for vocab creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_songs = playlist_songs.map(lambda x: x[\"song_name\"])\n",
    "for r in all_songs:\n",
    "    print(r)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = playlist_songs.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(pl_songs_count)\n",
    "# test = shuffled.skip(pl_songs_count-10000).take(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train))\n",
    "for t in train:\n",
    "    print(t)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_names = all_songs.batch(2048)\n",
    "u_ids = playlist_songs.map(lambda x: x[\"pid\"])\n",
    "playlist_ids = u_ids.batch(2048)\n",
    "\n",
    "# We need unqiue variables for upcoming vocabs\n",
    "unique_songs = np.unique(np.concatenate(list(song_names)))\n",
    "unique_pids = np.unique(np.concatenate(list(playlist_ids)))\n",
    "\n",
    "print(len(unique_pids))\n",
    "print(unique_pids[:5])\n",
    "print(len(unique_songs))\n",
    "unique_songs[10:20]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 32\n",
    "\n",
    "pid_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "      vocabulary=unique_pids, mask_token=None),\n",
    "  # We add an additional embedding to account for unknown tokens.\n",
    "  tf.keras.layers.Embedding(len(unique_pids) + 1, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "      vocabulary=unique_songs, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(unique_songs) + 1, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usongs = tf.data.Dataset.from_tensor_slices(unique_songs)\n",
    "metrics = tfrs.metrics.FactorizedTopK(\n",
    "  candidates=usongs.batch(4096).map(song_model)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = tfrs.tasks.Retrieval(\n",
    "  # metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self, pid_model, song_model, task):\n",
    "    super().__init__()\n",
    "    self.song_model: tf.keras.Model = song_model\n",
    "    self.pid_model: tf.keras.Model = pid_model\n",
    "    self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    \n",
    "    pid_embeddings = self.pid_model(features[\"pid\"])\n",
    "    positive_song_embeddings = self.song_model(features[\"song_name\"])\n",
    "\n",
    "    # The task computes the loss and the metrics.\n",
    "    return self.task(pid_embeddings, positive_song_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the above 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SongModel(pid_model, song_model, task)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "\n",
    "cached_train = train.shuffle(100_000).batch(2048).cache()\n",
    "# cached_test = test.batch(1024).cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(cached_train, epochs=10, verbose=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Recommendation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure we don't recommend the same song multiple times by parsing a unqiue list to recommend from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usongs = tf.data.Dataset.from_tensor_slices(unique_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = tfrs.layers.factorized_top_k.BruteForce(model.pid_model, k =1000)\n",
    "index.index_from_dataset(\n",
    "  tf.data.Dataset.zip((usongs.batch(32), usongs.batch(32).map(model.song_model)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, titles = index(tf.constant([\"50000\"]))\n",
    "print(f\"Recommendations for user: {titles[0, :1000]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "path = os.path.join(cwd, \"model\")\n",
    "tf.saved_model.save(index, path)\n",
    "\n",
    "cwd = os.getcwd() + \"\\\\model_weights\"\n",
    "path = os.path.join(cwd, \"model_weights\")\n",
    "model.save_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.getcwd() + \"\\\\model_weights\" + \"\\\\unique_songs.txt\"\n",
    "textfile = open(filename, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "for us in unique_songs:\n",
    "    textfile.write(us.decode(\"utf-8\"))\n",
    "    textfile.write(\"\\n\")\n",
    "\n",
    "textfile.close()\n",
    "\n",
    "filename = os.getcwd() + \"\\\\model_weights\" + \"\\\\unique_pids.txt\"\n",
    "textfile = open(filename, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "for us in unique_pids:\n",
    "    textfile.write(us.decode(\"utf-8\"))\n",
    "    textfile.write(\"\\n\")\n",
    "\n",
    "textfile.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae0582920bd24d03b21545c5279ef39598b67f1ebcb1fd7a047e45a89e1b0f1b"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('tf-gpu-2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
